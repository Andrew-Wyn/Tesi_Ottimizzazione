\documentclass[a4paper, 12pt]{article}
\usepackage[osf]{libertinus} %font generale del documento
\pagestyle{plain} %nessxun heading o foot particolare
\usepackage[fontsize=13pt]{scrextend} %dimensione font 
\usepackage[a4paper,top=3cm,bottom=3cm,left=3cm,right=3cm]{geometry} %impaginazione e margini documento
\usepackage{graphicx, wrapfig} %gestione immagini e grafiche
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usepackage{amsfonts}

\newtheorem{theorem}{Teorema}
\newtheorem{definition}{Definizione}
\newtheorem{prop}{Proposizione}
\newtheorem{corollary}{Corollario}

\begin{document}

\begin{titlepage} %crea l'enviroment
\begin{figure}[t] %inserisce le figure
    \centering\includegraphics[width=0.25\textwidth]{logo_unipg}
\end{figure}
\vspace{20mm}

\begin{Large}
 \begin{center}
	\textbf{Dipartimento di Matematica e Informatica\\ Corso di Laurea Triennale in Informatica\\}
	\vspace{20mm}
    {\LARGE{TESI DI LAUREA}}\\
	\vspace{10mm}
	{\huge{\bf Metodi di ottimizzazione su varietà differenziabili per la media di Fréchet su disco di Poincaré}}\\
\end{center}
\end{Large}


\vspace{30mm}
%minipage divide la pagina in due sezioni settabili
\begin{minipage}[t]{0.47\textwidth}
	{\large{\bf Relatore:\\ Prof. Bruno Iannazzo}}
\end{minipage}
\hfill
\begin{minipage}[t]{0.47\textwidth}\raggedleft
	{\large{\bf Candidato: \\ Luca Moroni\\ }}
	\vspace{5mm}
	{\large{\bf Matricola: \\ 311279\\ }}
\end{minipage}

\vspace{25mm}

\hrulefill

\vspace{5mm}

\centering{\large{\bf Anno Accademico 2020/2021 }}

\end{titlepage}

\renewcommand{\contentsname}{Contenuti}
\tableofcontents

\newpage

\section{Introduzione}
Nella seguente trattazione verra presentato il problema del minimo della funzione media di frechet sulla varietà disco di poincare, verra proposta una metodologia risolutiva che consiste nel trasportare il problema su una varieta conforme definita iperboloide, nella quale alcune formule per dimensione arbitraria sono computazionalmente meno onerose (e numericamente piu stabili ?)

\section{Metodi di ottimizzazione su $R^n$ e Varieta differenziabili}
In tale sezione andremo ad esplicitare le metodologie generali per trovare un minimo di una funzione $f$ definita su una varieta differenziabile $M$ a valori reali (notiamo che $R^n$ è un caso particolare di varietà differenziabile).\\
Sia $f: \mathbb{R} ^n \to \mathbb{R}$ non vincolata.
Un vettore $x^\ast$ è un minimo locale non vincolato per la funzione $f$ se (informalmente) $f$ ha un valore in $x^\ast$ non più grande di un intorno di $x^\ast$ stesso. Più formalmente $x^\ast$ è minimo locale per $f$ se esiste $\epsilon > 0$ tale che\\
\[f(x^\ast) \leq f, \mbox{ per ogni } x \mbox{ con } \parallel x - x^\ast \parallel < \epsilon\]\\
Un vettore $x^\ast$ è un minimo globale non vincolato rispetto ad $f$ se ha un valore in $f$ non più grande di ogni altro vettore, più formalmente è tale che\\\\
$f(x^\ast) \leq f \quad \forall x \in R^n$\\\\

\subsection{Condizioni necessarie per l'ottimalità}
Se la funzione di costo $f$ è differenziabile, possiamo utilizzare il gradiente e l'espansione in serie di Taylor per comparare il costo di un vettore con il costo di un suo intorno.
Ci aspettiamo perciò che se $x^\ast$ è un minimo non vincolato allora la variazione della funzione al primo ordine per una piccola variazione $\Delta x$ è non negativa\\\\
$\nabla f ( x^\ast )' \Delta x \geq 0$\\\\
perciò dovendo valere per $\Delta x$ sia positivi che negativi abbiamo la seguente condizione necessaria\\\\
$\nabla f ( x^\ast )' = 0$\\\\
Ci aspettiamo inoltre che anche la variazione della funzione al secondo ordine per una piccola variazione $\Delta x$ è non negativa\\\\
$\nabla f ( x^\ast )' \Delta x + (1/2) \Delta x' \nabla ^2 f (x^\ast) \Delta x\geq 0$\\\\
dato che la precedente osservazione ha imposto $\nabla f (x^\ast)' \Delta x = 0$ otteniamo che\\\\
$\Delta x' \nabla ^2 f (x^\ast) \Delta x\geq 0$\\\\
e che perciò\\\\
$\nabla ^2 f (x^\ast)$: semidefinita  positiva
\begin{prop}[Condizioni necessarie di ottimalità]
Sia $x^\ast$ un minimo locale non vincolato di $f:R^n \to R$, assumiamo $f$ differenziabile con continuità in un insieme aperto $S$ contenente $x^\ast$. Allora\\
$\nabla f(x^\ast) = 0$\\
Se $f$ è due volte differenziabile con continuità in $S$, allora\\
$\nabla^2 f(x^\ast)$: semi-definita positiva
\end{prop}
\subsection{Il caso della convessità}
Se la funzione $f$ è convessa non ci sono distinzioni tra un minimo locale ed un minimo globale, tutti i punti di minimo locale sono punti di minimo globale. Perciò un fatto importante è che la condizione $\nabla f(x^\ast) = 0$ è sufficiente per l'ottimalità, la dimostrazione è basata sulle proprietà di base della convessità della funzione $f$.
\begin{prop}[Funzioni Convesse] Sia $f:R^n \to R$ una funzione convessa su un insieme convesso $X$.
\begin{itemize}
  \item Un minimo locale di $f$ su $X$ è anche un minimo globale su $X$. Se inoltre $f$ è strettamente convessa, allora esiste al più un minimo per $f$.
  \item Se $f$ è convess e l'insieme $X$ è aperto, allora $\nabla f(x^\ast) = 0$ è una condizione necessaria e sufficiente per il vettore $x^\ast \in X$ per essere minimo globale di $f$ su $X$.
\end{itemize}
\end{prop}

\subsection{Condizioni Sufficienti per L'ottimalità}
Non è difficile trovare esempi in cui le condizione di ottimalita necessarie definite nelle sezione precedente
[$\nabla f(x^\ast) = 0 \quad \& \quad \nabla^2 f(x^\ast) \geq 0$]
portino ad identificare punti non di minimo locale come ad esempio punti di sella o punti di massimo.\\
Supponiamo di avere un vettore $x^\ast$ che soddisfa le seguenti condizioni\\\\
- $\nabla f(x^\ast) = 0$\\
- $\nabla^2 f(x^\ast)$: definita positiva\\\\
Allora abbiamo che per ogni $\Delta x \neq 0$\\\\
$\Delta x' \nabla^2f(x^\ast) \Delta x > 0$\\\\
Ciò implica che in $x^\ast$ la variazione di $f$ al secondo ordine data da un piccolo spostamento $\Delta x$ è positiva, perciò $f$ tende ad incrementare nell'intorno di $x^\ast$ e ciò implica che le due condizioni di cui sopra sono necessarie e sufficienti per l'ottimalità locale di $x^\ast$.
\begin{prop}[Condizioni di ottimalità sufficienti del secondo ordine] Sia $f:R^n \to R$ doppiamente differenziabile con continuità in un insieme aperto $S$. Si supponga esistere $x^\ast \in S$ che soddisfi le condizioni\\
$\nabla f(x^\ast) \quad \nabla^2 f(x^\ast)$: definita positiva\\
allora, $x^\ast$ è un minimo locale non vincolato di $f$. In particolare esistono $\gamma > 0$ e $\epsilon > 0$ tali che\\
\[ f(x) \geq f(x^\ast) + \gamma/2\parallel x - x^\ast \parallel^2, \quad \forall x \quad con \parallel x - x^\ast \parallel < \epsilon  \]
\end{prop}
\subsection{Discesa del gradiente in $R^n$}
Andiamo ora a definire nel dettaglio le principali metodologie computazionali di ottimizzazione non lineare su $R^n$. Tali metodologie sarannno trattate con un occhio alle possibili applicazioni.\\
Consideriamo il problema di trovare il minimo non vincolato di una funzione $f:R^n \to R$ la risoluzione analitica è infattibile, per problemi pratici, perciò l'approccio adottato è quello di applicare un algoritmo iterativo chiamato \textit{iterative descent} che opera come segue: prendiamo un vettore iniziale $x0$ e successivamente si genera una sequenza $x1, x2, ...$ tali che $f$ decresce ad ogni iterazione $f(x^{k+1}) < f(x^k)$ e sperabilmente raggiungere un punto di minimo.\\
I principali algoritmi di discesa sono Metodi del Gradiente poichè basano le decisioni in base al gradiente della funzione $f$.
\subsubsection{Selezionare la direzione di discesa}
Sia $v \in R^n-0$, $v^\perp$ è un iperpiano che divide lo spazio in due componenti connesse:
\begin{itemize}
  \item $v'd > 0$ (semipiano)
  \item $v'd < 0$ (semipiano)
  \item $v'd = 0$ (iperpiano)
\end{itemize}
se $v = \nabla f(x)$ ogni d tale che $v'd > 0$ è una direzione di decrescita.
\begin{theorem}
Sia $f \in C^1(\Omega), \Omega$ aperto di $R^n$ e sia $x \in \Omega$ e $\nabla f(x) \neq 0$ $\forall d \in R^n$ tale che $\nabla f(x)'d < 0$  $\exists \alpha 0 > 0$ tale che $f(x + \alpha d) < f(x)$ con $\alpha \in (0, \alpha0]$
\end{theorem}
Detto cio possiamo esplitare una formula che definisce una classe di algoritmi\\
$x^{k+1} = x^k + \alpha^k d^k \quad k=0, 1, ...$\\
Dove se $\nabla f(x^k) \neq 0$ la direzione $d^k$ è scelto in modo tale che\\
$\nabla f(x^k)'d^k < 0$\\
Ci sono varie possibilita nella scelta di $d^k$ e di $\alpha^k$. Molti metodi di gradiente sono definiti nella forma 
$x^{k+1} = x^k - \alpha^k D^k \nabla f(x^k)$,
dove $D^k$ è una matrice definita positiva e con $d^k = -D^k\nabla f(x^k)$ allora è diretto che $\nabla f(x^k)'d^k < 0$.\\
A seconda della scelta di $D^k$ abbiamo differenti metodologie applicabili.\\\\
\textbf{Steepest Descent}:\\
$D^k = I, \quad k = 0, 1, ...$\\\\
\textbf{Metodi di Newton}:\\
$D^k = (\nabla^2 f(x^k))^{-1}, \quad k = 0, 1, ...$\\
$\nabla^2 f(x^k)$ deve essere definita positiva, proprietà sempre verificata se $f$ è convessa.\\\\
\textbf{Steepest Descent Riscalata}:\\
$D^k$ matrice diagonale.\\\\
\textbf{Metodi di Schamsky}:\\
$D^k = (\nabla^2 f(x^0))^{-1}$\\\\
\textbf{Metodi di quasi Newton}:\\
$D^k = H(x^k) \approx \nabla^2 f(x^k)$\\
I metodi di quasi newton sono utilizzati nella pratica dal momento in cui il calcolo della matrice hessiana è computazionalmente oneroso, in tal caso la matrice $D^k$ è scelta in modo tale che la direzione $d^k = -D^k\nabla f(z^k)$ tenda ad approssimare la direzione del metodo di Newton. L'idea fondamentale riguardo questa metodologia è che tramite due iterate successive $k$, $k+1$ ed i valori $x^k$, $x^{k+1}$, $\nabla f(x^k)$, $\nabla f(x^{k+1})$ noi possiamo avere accesso a delle informazioni riguardo la matrice essiana, in particolare abbiamo che\\
$(\nabla f(x^{k+1}) - \nabla f(x^k) \approx H(x^{k+1})(x^{k+1} - x^k)$ che sappiamo essere un uguaglianza nel caso di $f$ quadratica. Definiti $p^k = x^{k+1} - x^k$ e $q^k = \nabla f(x^{k+1}) - \nabla f(x^k)$ allora $D^{k+1}$ deve soddisfare l'uguaglianza $D^{k+1} p^k = q^k$, questo prende il nome di metodo delle secanti (equazione delle secanti).

\subsubsection{Selezione del passo}
Ci sono numerose metodologie per la scelta del passo $a^k$ nei metodi del gradiente, ne listiamo alcune.\\\\
\textbf{Ricerca lineare esatta}:\\
scegliamo $\alpha^k$ tale che minimizza la funzione costo $f$ lungo la direzione $d^k$, percio\\
$f(x^k + \alpha^k d^k) = \min_{\alpha \geq 0} f(x^k + \alpha d^k)$.\\\\
\textbf{Ricerca lineare esatta limitata}:\\
Questa è una versione modifica della metodologia precedente più semplice da implementare in vari casi. definito un scalare $s > 0$ $\alpha^k$ è scelto in modo tale che minimizza il costo di $f$ nell'intervallo $[0, s]$\\
$f(x^k + \alpha^k d^k) = \min_{\alpha \in [0, s]} f(x^k + \alpha d^k)$.\\\\
Le due metodologie appena presentate sono solitamente implementate con l'aiuto delle tecniche risolutive dell'ottimizzazione in una variabile, che è un problema certamente più facile da risolvere in modo analitico.\\\\
\textbf{Metodo di armijo}:\\
scegliere $\alpha^k$ in modo che garantisca una decrescita sufficiente a garantire la convergenza del metodo sotto opportune ipotesi. Il metodo di armijo è stato definito come segue. Siano fissi $s, \beta$ e $\sigma$ con $0 < \beta < 1$, e $0 < \sigma < 1$ e sia $\alpha^k = \beta^{m_k} s$, dove $m_k$ è il primo intero non negativo per cui vale\\
$f(x^k) - f(x^k + \beta^m s d^k) \geq -\sigma \beta^m s \nabla f(^k)'d^k$.\\
Solitamente si scelgono i valori come segue, $\sigma \in [10^{-5}, 10^{-1}]$, il fattore di riduzione $\beta \in [1/10, 1/2]$, possiamo scegliere $s = 1$ e moltiplicare la direzione $d^k$ per uno scalare.\\
Per questa metodologia diamo anche il seguente teorema che rappresenta un risultato importante.
\begin{definition}
Una sequenza di direzioni $\{x^k\}_k$ è detta limitata se vale la seguente $\limsup\limits_{k \in K} \nabla f(x)'d^k < 0$
\end{definition}
\begin{definition}
Una successione di direzioni $\{x^k\}_k, \quad \{d^k\}_k$ è detta \textbf{gradient related} se per ogni sotto-successione di $\{x^k\}_k$ che converge ad un punto non stazionario $\{d^k\}_k$ è limitata. Perciò non sarà mai ortogonale al gradiente (sennò sarebbe stazionario)
\end{definition}
\begin{theorem}
Sia $\{x^k\}_k$ una sequenza infinita (non definitivamente uguale al suo limite) generata dal metodo $x^{k+1} = x^k + \alpha^k d^k$, dove $\alpha^k$ è generato con la regola di armijo e $d^k$ è \textbf{gradient related}. Allora il limite è un punto stazionario.
\end{theorem}
\textbf{Metodo a passo costante}:\\
Si definisce un passo costante $s > 0$ e si fissa $\alpha^k = s, \quad k = 0,1, ...$\\
Il passo costante è una metodologia veramente semplice da implementare ma si deve fare attenzione nella scelta del passo, se si sceglie un valore di $s$ troppo grande allora il metodo divergerà, contrariamente se il passo $s$ venisse scelto troppo piccolo l'ordine di convergenza risulterebbe troppo lento.\\\\
\textbf{Metodo di diminuzione del passo}:\\
Scegliamo il passo in modo tale che $\alpha^k \rightarrow 0$. Una problematica legata a questa metodologia è che il passo può diventare troppo piccolo per cui non può essere garantita la convergenza, per questa ragione viene richiesto che $\sum_{n=1}^{\infty} \alpha^k = \infty$

\subsection{Discesa del gradiente su varietà}
Ora che abbiamo un contesto teorico e una consapevolezza di cosa voglia dire effetutare ottimizzazione su uno spazio euclideo n-dimensionale ($R^n$), possiamo approcciare le metodologie computazionali per trovare un minimo non vincolato di una funzione $f:M \to R$ analoghe a quella definite nella sezione precedente. Prima di andare nel vivo del discorso dobbiamo però munirci degli strumenti necessari, richiamiamo la parte dedicata alle varietà differenziabili presente nell'appendice e aggiungiamo quanto segue.

\subsubsection{Retrazioni}
Su varietà differenziabili la nozione di muoversi nella direzione del vettore tangente rimanendo nella varietà stessa è generalizzato dal concetto di \textbf{Retrazione}, concettualmente una retrazione $R$ su $x$, denominata come $R_x$, è una mappa che va da $T_xM$ (spazio tangente in $x$ rispetto a $M$) in $M$ con una condizione di rigidità locale che preserva il gradiente in $x$.
\begin{definition}[Retrazione]
Una retrazione in una varietà $M$ è una mappa liscia $R$ che ha come dominio l'insieme dei $T_xM \quad \forall x \in M$ in $M$ con le seguenti proprietà. Sia $R_x$ la restrizione di $R$ a $T_xM$.
\begin{itemize}
  \item $R_x(0_x) = x$, dove $0_x$ denota l'elemento zero (origine) di $T_xM$.
  \item Con l'identificazione canonica $T_{0_x} T_xM \simeq T_xM$, $R_x$ soddisfa\\
  $DR_x(0_x) = id_{T_xM},$\\
  dove $id_{T_xM}$ denota la mappa identità su $T_xM$.
\end{itemize}
\end{definition}
Ci riferiamo alla condizione $DR_x(0_x) = id_{T_xM}$ come condizione di \emph{rigidità locale}. Equivalentemente, per ogni vettore tangente $\xi$ in $T_xM$, la curva $\gamma_{\xi}:t \mapsto R_x(t\xi)$ soddisfa $\gamma_{\xi}(0)' = \xi$.\\
Oltre a trasformare elementi di $T_xM$ in elementi di $M$, un secondo importate scopo della retrazione $R_x$ è quello di trasformare una funzione costo ($f:M \to R$) definita in un intorno di $x \in M$ in una funzione costo definita sullo spazio vettoriale $T_xM$. Nello specifico, data una funzione reale $f$ su una varietà $M$ su cui è definita una retrazione $R$, abbiamo che $\hat{f} = f \circ R$ denota il $\emph{pullback}$ di $f$ attraverso $R$. Per $x \in M$, abbiamo che\\
$\hat{f_x} = f \circ R_x$\\
denota la restrizione di $\hat{f}$ su $T_xM$. Si noti che $\hat{f_x}$ è una funzione reale su uno spazio vettoriale. Dalla condizione di rigidità locale abbiamo che $D\hat{f_x}(0_x) = Df(x)$. Se $M$ è dotato di una metrica Riemanniana abbiamo\\
grad $\hat{f_x}(0_x) =$grad $f(x)$.\\
In alcuni casi può essere necessario dover effettuare operazioni tra vettori appartenenti a spazi tangente diffenti, come ad esempio nel caso dei metodi di ottimizzazzione quasi Newton. Nell'ottimizzazione basata su retrazioni lo strumento standard per trasportare vettori appartenenti ad uno spazio tangente in un altro spazio tangente è il $trasporto vettoriale$. Informalmente, dati due vettori tangenti ad $x$, chiamati $v_x, w_x \in T_xM$, un trasporto vettoriale $\mathcal{T}$ associato ad una retrazione $R$, è una mappa liscia che genera un vettore appartenente ad $T_{R_x(w_x)}M$. Per semplicità utilizzeremo la notazione $\mathcal{T}_{x \to y}(v)$.\\
Un caso speciale di trasporto vettoriale è il $trasporto$ $parallelo$ che puo essere interpretato come un trasporto vettoriale la cui retrazione associata è la mappa esponenziale.
\subsubsection{Metodi \emph{line-search}}
I metodi di ricerca in linea (definiti \emph{line-search}) su varietà si basano sulla formula di aggiornamento\\
$x^{k+1} = R_{x^k}(t^k \eta^k)$,\\
dove $\eta^k$ è in $T_xM$ e $t^k$ è uno scalare. Una volta scelta la retrazione $R$ ci rimane da decidere la direzione $\eta^k$ e la lunghezza del passo $t^k$. Per ottenere la convergenza del metodo delle restrizioni sulla scelta di questi due parametri devono essere fatte.\\
\begin{definition}[sequenza gradient related su varietà]
Data una funzione di costo $f$ su una varietà riemanniana $M$, una sequenza $\{\eta^k \}$, $\eta^k \in T_xM$ è \textbf{gradient related} se per ogni sotto-sequenza $\{x^k\}_{k \in K}$ di $\{x^k\}$ che converge ad un punto non stazionario di $f$, la corrispondente sotto-sequenza $\{\eta^k\}_{k \in K}$ è limitata e soddisfa\\
$\limsup\limits_{k \to \infty, k \in K} \langle$grad $f(x^k), \eta^k \rangle < 0$
\end{definition}
\begin{definition}[Punto di Armijo su varietà]
Data una funzione $f$ su una varietà riemanniana $M$ dotata di una retrazione $R$, un punto $x \in M$, un vettore tangente $\eta \in T_xM$, e degli scalari $\bar{\alpha} > 0, \beta, \sigma \in (0, 1)$, un \textbf{punto di Armijo su varietà} è $\eta^A = t^{A}\eta = \beta^{m}\bar{\alpha}\eta$, dove $m$ è il più piccolo intero non negativo tale che\\
$f(x) - f(R_x(\beta^{m}\bar{\alpha}\eta)) \geq -\sigma \langle$grad $f(x), \beta^m\bar{\alpha}\eta \rangle_x$.\\
Il valore reale $t^A$ è il \textbf{Passo di Armijo}.
\end{definition}
Andiamo ora a definire un algoritmo generico per la discesa di gradiente su varietà (\textbf{Accelerated Line Search}), tale algoritmo rappresenta una generalità di metodologie di ottimizzazione delineando però, nel contempo, delle restrizioni fondamentali che garantiscono la convergenza della sequenza di punti generati.
\begin{algorithm}[H]
\SetAlgoLined
\KwIn{Manifold $M$; funzione costo $f$ differenziabile definita su $M$; retraction $R$; scalari $\bar{\alpha} > 0$, $c, \beta, \sigma \in (0,1)$; initial iterate $x^0 \in M$}
\KwOut{Sequenza $\{x^k\}$}
\For{$k=0, 1, 2, ...$}{
    Prendere $\eta^k \in T_xM$ tale per cui la sequenza $\{\eta^i\}_{i=0,1,...}$ sia \emph{gradient related}.\\
    Selezionare $x^{k+1}$ tale che
    \begin{equation}
    \label{ctr-als}
    \hspace{10mm}f(x^k)-f(x^{k+1}) \geq c(f(x^k)-f(R_{x^k}(t^{k^A}\eta^k)))
    \end{equation}
    Dove $t^{k^A}$ è il passo definito dalla regola di Armijo su varietà definita poco sopra.
}
\caption{Accelerated Line Search (ALS)}
\end{algorithm}

Dallo pseudocodice appena esplicitato è chiaro che tale algoritmo puo essere visto come una generalizzazione della scelta del passo di Armijo descritto nella sotto-sezione 2.2, difatti scegliere $x^{k+1} = R_{x^k}(t^{k^A}\eta^k)$ soddisfa \eqref{ctr-als}. Inoltre la condizione rilassata \eqref{ctr-als} ci permette un ampio spazio di manovra nella scelta di $x^{k+1}$ grantendoci la convergenza ad un punto stazionario, come enunceremo formalmente, e ciò puo portare alla definizione di algoritmi altamente efficienti.

\subsubsection{Convergenza}
Il concetto di convergenza puo essere ridefinito e generalizzato su varietà. Una sequenza infinita $\{x^k\}_{k=0,1,...}$ di punti su una varietà $M$ è detta essere convergente se esiste una carta $(U, \psi)$ di $M$, un punto $x^\ast \in M$ e $K > 0$ tali che $x^k$ è in $U$ per ogni $k \geq K$ e tale che la sequenza $\{\psi(x^k)\}_{x=K, K+1, ...}$ converge a $\psi(x^\ast)$ (in tal caso applichiamo il concetto di convergenza di $R^n$, avendo $\psi$ come codominio uno spazio euclideo). Il punto $\psi^{-1}(\lim_{k \to \infty} \psi(x^k))$ è chiamato il \emph{limite} della sequenza $\{x^k\}_{k=0,1,...}$. Tutte le sequenze convergenti di una vairetà di \emph{Hausdorff} hanno uno ed un solo punto limite.\\
Diamo ora un importante risultato rispetto alla convergenza dell'algoritmo \emph{ALS}, tale enunciato deriva direttamente dal risultato di convergenza definito nella sezione 2.2 rispetto al passo di Armijo in $R^n$, in tal caso però viene data una generalizzazione di quest'ultimo essendo, come già detto, ALS una generalizzazione del passo di Armijo e lavorando non più su $R^n$ ma su varietà differenziabili.
\begin{theorem}
Sia $\{x^k\}$ una sequenza infinita di iterate generate da ALS. Allora ogni punto di accumulazione  di $\{x^k\}$ è un punto stazionario della funzione costo $f$
\end{theorem}
Inoltre assumendo compattezza della varietà $M$ possiamo enunciare il seguente
\begin{corollary}
Sia $\{x^k\}$ una sequenza finita di iterate generata da ALS. Assumiamo che l'insieme di livello $L = \{x \in M : f(x) \leq f(x^0)\}$ è compatto allora\\ $\lim_{k \to \infty} \parallel $grad $f(x^k) \parallel = 0$.
\end{corollary}

\section{Disco di poincare e Iperboloide}
Lo spazio iperbolico può essere definito in vari modi equivalenti, noi descriveremo l'iperboloide ed il disco di poincare, ciascuno di questi è un modello dello spazio iperbolico ed inoltre nessuno dei due è prevalente in letteratura.\\
\begin{figure}[t] %inserisce le figure
    \centering\includegraphics[width=0.50\textwidth]{HyperboloidProjection.png}
    \caption{Rappresentazione della proiezione stereografica tra disco e iperboloide}
\end{figure}
\subsection{Iperboloide}
Possiamo definire $H^n$ come il luogo dei punti di norma -1 in $R^{n:1}$, dotato dell'usuale prodotto di minkowski $(\langle u, v \rangle_{n:1} = \sum_{i=1}^{n} u_iv_i - u_{n+1}v_{n+1}; \quad u,v \in R^{n+1})$, questo luogo di punti ha in realta due componenti connesse, noi ne scegliamo una.
\begin{definition}[Iperboloide]
Il modello dell'iperboloide è definito nel modo seguente.\\
$H^n = \{x \in R^{n+1} | \langle x, x \rangle_{n:1} = -1; \quad x_{n+1} > 0 \}$
\end{definition}
Lo spazio tangente in un punto $p \in H^n$ è $T_pH^n = \{x \in R^{n:1} | \langle p,x \rangle_{n:1} = 0\}$.\\
Possiamo asserire che l'iperboloide è una varietà riemanniana. Andiamo ora ad esplicitare gli strumenti necessari per effettuare ottimizzazione su questa varietà.
\subsubsection{Metrica}
La metrica, definita nel $Tangent Bundle$ dell'iperboloide, è la metrica indotta dal prodotto di Minkowski già esplicitata.
\subsubsection{Distanza}
La distanza tra due punti dell'iperboloide rappresenta la lunghezza della geodedica che unisce i due punti, è definita come segue.\\
$d_{H^n} (u,v) = arccosh(-\langle u, v \rangle_{n:1}); \quad u,v \in H^n$
\subsubsection{Mappa Esponenziale}
La mappa esponenziale definita su $p \in H^n$ e $v \in T_pH^n$, che ha proprietà di retrazione. La mappa esponenziale rappresenta la curva geodedica calcolata nel punto $1$ originaria in $p$ con derivata in $0$ punto pari a $v$. E' definita come segue.\\
$Exp_p(v) = cosh(\parallel v \parallel_{n:1})p + sinh(\parallel v \parallel_{n:1}) v / \parallel v \parallel_{n:1}$
\subsubsection{Gradiente della funzione distanza}
La deifinize dell'iperboloide e dello spazio di minkowski nel quale è costruito ci aiuta nel calcolo del gradiente della funzione distanza.\\
Data una funzione costo $E$ qualsiasi possiamo calcolarne il gradiente in un punto $p \in H^n$, anch'esso arbitrario, passando dapprima per il gradiente definito nell'ambiente, in tal caso lo spazio di minkowski, ricordiamo che quest'ultimo è cosi definito.\\
$\nabla_{p}^{R^{n:1}} E = (\frac{\partial E}{\partial x_1}|_p, ..., \frac{\partial E}{\partial x_n}|_p, -\frac{\partial E}{\partial x_{n+1}}|_p)$\\
Una volta calcolato il gradiente nell'ambiente procediamo a proiettarlo nello spazio tangente del punto $p \in H^n$ come segue.\\
$\nabla_{p}^{H^n} E = \nabla_{p}^{R^{n:1}} E + \langle p, \nabla_{p}^{R^{n:1}} E \rangle_{n:1} p$.\\
Relativamente al problema della media di fréchet che andremo ad esplicitare definiamo la seguente formula rappresentante il gradiente su spazio di minkowski della funzione distanza.\\
$\nabla_u^{R^{n:1}} d_{H^n}(u,v) = - (\langle u,v \rangle_{n:1}^2 - 1)^{-1/2} v$
\subsection{Disco di Poincaré}
Andiamo ora a definire l'altro modello dello spazio iperbolico, rappresentato come tutti i punti di norma strettamente minore di 1 in $R^n$.
\begin{definition}[Disco di Poincaré]
Il modello del disco di poincare è definito nel modo seguente\\
$D^n = \{x \in R^n | \parallel x \parallel < 1\}$
\end{definition}
Essendo $D^n$ un sottoinsieme di $R^n$ lo spazio tangente in ogni punto di $D^n$ è $R^n$. Come fatto per l'iperboloide andiamo ad esplicitare i vari strumenti necessari per effettuare ottimizzazione.
\subsubsection{Metrica}
La metrica, definita nello spazio tangente del disco di poincaré $T_pD^n$ rispetto ad un punto $p \in D^n$, è la seguente.\\
$g^D = \lambda_p^2 g^E$; con $\lambda_p = \frac{2}{1- \parallel p \parallel^2}$ e con $g^E$ la metrica euclidea.
\subsubsection{Distanza}
La distanza tra due punti del disco di Poincaré rappresenta la lunghezza della geodedica che unisce i due punti, è definita come segue.\\
$d_{D^n}(u, v) = arccosh(1 + \frac{\parallel u - v\parallel^2}{(1-\parallel u \parallel^2)(1-\parallel v \parallel^2)})$
\subsubsection{Mappa Esponenziale}
La mappa esponenziale definita su $p \in D^n$ e $v \in T_pD^n$. La mappa esponenziale rappresenta la curva geodedica calcolata nel punto $1$ originaria in $p$ con derivata in $0$ punto pari a $v$. E' definita come segue.\\
$Exp_p(v) = \frac{\lambda_p (cosh(\lambda_p \parallel v \parallel) + \langle p,\frac{v}{\parallel v \parallel} \rangle sinh(\lambda_p \parallel v \parallel))p}{1 + (\lambda_p - 1) cosh(\lambda_p \parallel v \parallel) + \lambda_p \langle p, \frac{v}{\parallel v \parallel} sinh(\lambda_p \parallel v \parallel)} + \frac{\frac{1}{\parallel v \parallel} sinh(\lambda_p \parallel v \parallel) v}{1 + (\lambda_p - 1) cosh(\lambda_p \parallel v \parallel) + \lambda_p \langle p, \frac{v}{\parallel v \parallel} sinh(\lambda_p \parallel v \parallel)}$
\subsubsection{Gradiente della funzione distanza}
Dati $u,v \in D^n$ la formula del gradiente riemanniano della funzione distanza è dato da.\\
$\nabla_u^{E} d(u,v) = \frac{4}{b \sqrt{c^2 - 1}} (\frac{(\parallel v \parallel^2 - 2\langle u,v \rangle + 1)v}{a^2} - \frac{v}{a})$ con $a = 1 - \parallel u \parallel^2$, $b = 1 - \parallel v \parallel^2$, $c = 1 + \frac{2}{ab} \parallel u - v \parallel^2$.\\
Sappiamo che per per ricavare il gradiente riemanniano dal gradiente euclideo è sufficiente riscalare $\nabla^E$ per l'inverso della metrica riemanniana, perciò: $\nabla_u^{D^n} d(u,v) = \frac{\nabla_u^{E} d(u,v)}{\lambda_u^2}$
\subsection{Iperboloide come modello conforme}
Come inizialmente accennato le due varietà trattate fin'ora sono due modelli conformi dello spazio iperbolico ovvero, esiste un diffeomorfismo invertibile conforme (mantiene gli angoli ma non le lunghezze) che porta da una varietà all'altra. Il diffeomorfismo in questione è la proieizone stereografica $\rho$ definita come segue.\\\\
$\rho : H^n \to D^n$\\
$\rho(x) = \frac{1}{x_{n+1} + 1}(x_1, ..., x_n)$; con $x \in H^n$\\\\
$\rho^{-1} : D^n \to H^n$\\
$\rho^{-1}(y) = \frac{2}{1 - r}(y_1, ..., \frac{1+r}{2})$; con $r = \parallel y \parallel^2$ e $y \in D^n$\\\\
A questo punto facciamo vedere che vale la seguente uguaglianza, ci sarà utile nella sezione successiva.\\
$d_{D^n}(a, b) = d_{H^n}(\rho^{-1}(a), \rho^{-1}(b))$\\\\
Daremo due risultati, dimostreremo che l'uguaglianza è valida, dando dapprima una dimostrazione per $n = 2$, e poi la generalizzeremo per $n$ qualsiasi.\\\\
Caso $n=2$, siano $a, b \in D^2$\\
$a = (a_1, a_2)$\\
$b = (b_1, b_2)$\\
$\rho^{-1}(a) = \frac{2}{1-r_a}(a_1, a_2, \frac{1 + r_a}{2})$\\
$\rho^{-1}(b) = \frac{2}{1-r_b}(b_1, b_2, \frac{1 + r_b}{2})$\\
con $r_a = (a_1^2 + a_2^2)$ e $r_b = (b_1^2 + b_2^2)$\\\\
\begin{equation*}\begin{gathered}
d_{H^n}(\rho^{-1}(a), \rho^{-1}(b)) = \\
arccosh(-\langle (\frac{2a_1}{1-r_a}, \frac{2a_1}{1-r_a}, \frac{1+r_a}{1-r_a}), (\frac{2b_1}{1-r_b}, \frac{2b_1}{1-r_b}, \frac{1+r_b}{1-r_b}) \rangle_{2:1}) =\\
arccosh(- (\frac{4a_1b_1}{(1-r_a)(1-r_b)} + \frac{4a_2b_2}{(1-r_a)(1-r_b)} - \frac{(1+r_a)(1+r_b)}{(1-r_a)(1-r_b)}) =\\
arccosh(\frac{(1+r_a)(1+r_b) - 4a_1b_1 - 4a_2b_2}{(1-r_a)(1-r_b)}) =\\
arccosh(\frac{1 + r_a +r_b + r_ar_b - 4a_1b_1 - 4a_2b_2}{1 - r_a - r_b + r_ar_b}) = \\
arccosh(\frac{1 -r_a -r_b + 2r_a + 2r_b + r_ar_b - 4a_1b_1 - 4a_2b_2}{1 - r_a - r_b + r_ar_b}) = \\
arccosh(1 + \frac{2r_a + 2r_b - 4a_1b_1 - 4a_2b_2}{(1-r_a)(1-r_b)}) =\\
arccosh(1 + 2 \frac{r_a + r_b - 2a_1b_1 - 2a_2b_2}{(1-r_a)(1-r_b)}) = \\
arccosh(1 + 2\frac{\parallel a - b \parallel^2}{(1 - \parallel a \parallel^2)(1 - \parallel b \parallel^2)}) =\\
d_{D^n}(a, b).
\end{gathered}\end{equation*}
\\\\
Facciamo vedere ora che l'uguaglianza vale anche nel caso in cui $n$ è un valore generico, siano $a, b \in D^n$. Procediamo facendo vedere che i due termini dell'uguaglianza rappresentano la stessa quantità.\\
\begin{equation*}\begin{gathered}
d_{D}(a,b) = arccosh(1 + 2\frac{\parallel a + b \parallel^2}{(1 - \parallel a \parallel^2)(1 - \parallel b \parallel^2)})
\end{gathered}\end{equation*}
\begin{equation*}\begin{gathered}
d_{H}(\rho^{-1}(a),\rho^{-1}(b)) = arccosh(- \langle \rho^{-1}(a),\rho^{-1}(b) \rangle_{n:1})
\end{gathered}\end{equation*}
Facciamo vedere ora, che gli argomenti della funzione arccosh, nelle due equazioni riportate, rappresentano in realtà lo stesso valore.
\begin{equation*}\begin{gathered}
- \langle \rho^{-1}(a),\rho^{-1}(b) \rangle_{n:1} = \\
- \langle \frac{2}{1 - \parallel a \parallel^2} \begin{bmatrix} a \\ \frac{1 + \parallel a \parallel^2}{2} \end{bmatrix}, \frac{2}{1 - \parallel b \parallel^2} \begin{bmatrix} b \\ \frac{1 + \parallel b \parallel^2}{2} \end{bmatrix} \rangle_{n:1} = \\
\frac{4}{(1 - \parallel a \parallel^2)(1 - \parallel b \parallel^2)} \left( a'b - \frac{(1 + \parallel a \parallel^2)(1 + \parallel b \parallel^2)}{4} \right) = \\
\frac{-4a'b + 1 + \parallel a \parallel^2 + \parallel b \parallel^2 + \parallel a \parallel^2 \parallel b \parallel^2}{(1 - \parallel a \parallel^2)(1 - \parallel b \parallel^2)} = \\
\frac{-4a'b + 1 + 2\parallel a \parallel^2 + 2\parallel b \parallel^2 - \parallel a \parallel^2 - \parallel b \parallel^2 + \parallel a \parallel^2 \parallel b \parallel^2}{(1 - \parallel a \parallel^2)(1 - \parallel b \parallel^2)} = \\
1 + 2 \frac{\parallel a - b \parallel^2}{(1 - \parallel a \parallel^2)(1 - \parallel b \parallel^2)}
\end{gathered}\end{equation*}

\subsection{Media di Fréchet e problema del centroide}
\begin{definition}[Media di Fréchet]
La funzione media di Fréchet di p-punti è cosi definita.\\
$f(\Theta) = \frac{1}{p}\sum_{i=1}^p d^2(\Theta, x^{(i)})$
\end{definition}
Il problema di trovare il centroide di p-punti consiste nel trovare il punto che minimizza la funzione media di Fréchet definita a partire dai punti in questione, il cui minimo esiste ed è unico. Tale problema può essere risolto direttamente in uno spazio euclideo il cui minimo è dato dalla media aritmetica dei p-punti. Mentre in uno spazio iperbolico, come nel nostro caso, la soluzione non è diretta e perciò si applicano le metodologie di ottimizzazione su varietà differenziabili viste precedentemente.\\
Diamo ora una congettura che possiamo affermare essere corretta in funzione dell'uguaglianza $d_{D^n}(a, b) = d_{H^n}(\rho^{-1}(a), \rho^{-1}(b))$ che abbiamo verificato nella sezione precedente.\\
Dati $a^{(i)}, \Theta \in D^n$; con $i = 1...p$ dall'uguaglianza precedente è facile notare che $\frac{1}{p}\sum_{i=1}^p d_{D^n}^2(\Theta, a^{(i)}) = \frac{1}{p}\sum_{i=1}^p d_{H^n}^2(\rho^{-1}(\Theta), \rho^{-1}(a^{(i)}))$, perciò essendo $\rho$ un diffeomorfismo se $\exists \Theta \in D^n$ per cui $\frac{1}{p}\sum_{i=1}^p d_{D^n}^2(\Theta, a^{(i)})$ è minima allora esiste $\exists \Psi \in H^n$ per cui è minima $\frac{1}{p}\sum_{i=1}^p d_{H^n}^2(\Psi, \rho^{-1}(a^{(i)}))$. Essendo inoltre $f(x) = \hat{f} = f(\rho^{-1}(x))$ allora cercare il minimo di $f$ equivale a cercare il minimo di $\hat{f} \circ \rho^{-1}$.\\
In funzione di tale congettura possiamo trasportare il problema definito su disco di Poincaré nella varieta Iperboloide e viceversa, notiamo che le formule necessarie ad effettuare ottimizzazione sulla varietà iperboloide sono computazionalmente meno complesse rispetto a quelle definite per il disco di Poincaré.
\section{Metodi utilizzati con pseudocodice ed esperimenti}
In questa sezione andremo a definire lo pseudocodice degli algoritmi di ottimizzazione che abbiamo utilizzato per confrontare il calcolo del minimo della funzione media di Fréchet sui modelli dello spazio iperbolico descritti nel capitolo precedente, ovvero Disco di Poincare ed Iperboloide, sfrutteremo il fatto che questi due modelli sono tra di loro conformi, ovvero che esiste una mappa conforme che li lega, faremo vedere se uno dei due modelli è effettivamente conveniente da utilizzare.
\subsection{Algoritmi e Pseudocodice}
In tale sezione vedremo lo pseudocodice dei metodi utilizzati per i nostri esperimenti, descriveremo quattro algoritmi, di cui due sono algoritmi di prim'ordine \textbf{passo fisso} e \textbf{armijo}, ovvero che utilizzano informazioni solamente sul gradiente nel punto per decidere il passo e la direzione dell'iterazione, gli altri due \textbf{Barzilai-Borwein} ed \textbf{LBFGS - Limited memory BFGS} sono algoritmi di quasi Newton e perciò hanno accesso ad informazioni di approssimazione al secondo ordine approssimando la matrice hessiana, sfruttando l'equazione delle secanti. Tutti gli algoritmi si basano sul calcolo del gradiente della funzione da minimizzare, in tal caso la derivata della media di frechet che possiamo calcolare tramite la derivata riemanniana della funzione distanza esplicitata sia per il disco di Poincaré che per l'iperboloide nella sezione dedicata alle varietà utilizzate.
\subsubsection{Passo fisso}
L'algoritmo a Passo fisso è un algoritmo del primo ordine molto elementare, la direzione scelta è l'opposta della direzione del gradiente, tale algoritmo implementato su una varietà differenziabile necessita solamente del gradiente riemanniano della funzione da minimizzare, di una retrazione e di un parametro $\lambda$ che denota il passo fisso applicato ad ogni iterazione.\\\\
TODO: PSEUDOCODICE
\subsubsection{Armijo}
L'algoritmo di Armijo è anche esso un algoritmo del primo ordine, la direzione scelta è l'opposta della direzione del gradiente, per la selezione del passo si basa sul calcolo del punto di armijo riemanniano, descritto nella sezione dedicata all'ottimizzazione su varietà, per poter essere eseguito necessita, anche esso, del gradiente riemanniano della funzione da minimizzare, di una retrazione e di tre parametri $\gamma \in (0,1)$ , $\beta \in (0,1)$ e $s > 0$.\\\\
TODO: PSEUDOCODICE
\subsubsection{Barzilai-Borwein}
L'algoritmo di Barzilai-Borwein è un metodo quasi Newton e si basa sulla soluzione, per $k \geq 1$, del problema dei minimi quadrati, che ci permette di avere ricavare informazioni sulla matrice hessiana, quindi del secondo ordine.\\
\[\min_{t} \parallel s_kt - y_k \parallel_2 \],\\
con $s_k = x_{k+1} - x_k$ ed $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$, il quale, assumendo che $x_{k+1} \neq x_k$ ha un'unica soluzione $t = \frac{s_k'y_k}{s_k's_k}$. Quando $s_k'y_k > 0$ il passo viene scelto come.\\
\[a_{k+1}^{BB} = \frac{s_k's_k}{s_k'y_k}\].\\
Per tradurre lalgoritmo dalla sua definizione su spazio euclideo in una versione compatibile con ottimizzazione su varietà differenziabili dobbiamo andare a ridefinire $s_k$ ed $y_k$. Al passo $k+1$, l'hessiana è una mappa lineare da $T_{x_{k+1}}M$ a $T_{x_{k+1}}M$ perciò considereremo il vettore $-\alpha_kg_k \in T_{x_k}M$ e lo trasporteremo in $T_{x_{k+1}}M$, perciò\\
\[s_k = \mathcal{T}_{x_k \to x_{k+1}}(-\alpha_kg_k)\].\\
Per ottenere $y_k$ dobbiamo sottrarre due gradienti appartenenti a due differenti spazi tangente, perciò
\[y_k = g_{k+1} - \mathcal{T}_{x_k \to x_{k+1}}(g_k)\].\\
Mentre per quanto riguarda il prodotto scalare utilizzeremo quello riemanniano, quindi\\
\[a_{k+1}^{BB} = \frac{\langle s_k,s_k \rangle_{x_{k+1}}}{\langle s_k,y_k \rangle_{x_{k+1}}}\].\\
TODO: PSEUDOCODICE
\subsubsection{LBFGS}
L-BFGS è un algoritmo di quasi Newton e differisce dalla sua versione originale RBFGS nel calcolo della direzione $d_k = -H_kd_k$, la quale approssima la direzione di Newton ($H_k$ approssima l'inversa della matrice hessiana). Nella presente andremo a descrivere una delle implementazioni di L-BFGS chiamata \textbf{two loop recursion}. Definiamo dapprima l'algoritmo nella sua versione euclidea, assumiamo di avere in memoria $m$ aggiornamenti del tipo:\\
\begin{itemize}
    \item $s_k = x_{k+1} - x_k$.
    \item $y_k = g_{k+1} - g_k$.
\end{itemize}
Definiamo $p_k = \frac{1}{y_k's_k}$, ed $H_k^0$ l'approssimazione iniziale dell'inversa della matrice hessiana sulla quale l'algoritmo all'iterata $k$-esima si basa.\\
L'algoritmo fa fede al funzionamento di BGFGS per il calcolo di $H_{k+1}$ da $H_k$\\
\[H_{k+1} = (I - p_ks_k'y_k)H_k(I - p_ky_ks_k') + p_ks_ks_k'\].\\
Per un $k$ fissato definiamo una sequenza di vettori $q_{k-m}, ..., q_k$, con $q_i = (I - p_iy_is_i')q_{i+1}$ e $q_k = g_k$. Definiamo inoltre un'altra sequenza di vettori $z_{k-m}, ..., z_k$ con $z_i = H_iq_i$, definendo $z_{k-m} = H_k^0q_{k-m}$ e ricorsivamente $\beta_i = p_iy_i'z_i$ e $z_{i+1} = z_i + (p_is_i'q_{i+1} - \beta_i)s_i$ allora $z_k = H_kd_k$.
Nella versione riemanniana faremo un forte uso del trasporto vettoriale, ridefinendo dapprima $s_k$ ed $y_k$ come fatto nella versione rieamanniana dell'algoritmo Barzilai-Borwain\\
\[s_k = \mathcal{T}_{x_k \to x_{k+1}}(-\alpha_kg_k),\]\\
e\\
\[y_k = g_{k+1} - \mathcal{T}_{x_k \to x_{k+1}}(g_k),\]\\
ridefiniamo $p_k$ come segue\\
\[p_k = \frac{1}{\langle s_k,y_k \rangle_{x_{k+1}}},\]\\
Infine dobbiamo applicare il trasporto vettoriale a tutti i vettori $s_i$ e $y_i$ trasportandoli alla fine di ogni iterazione da $T_{x_k}M$ ad $T_{x_{k+1}}M$.\\
TODO: PSEUDOCODICE
\subsection{Esperimenti}
Gli esperimenti effettuati ci permetteranno di confrontare il problema di ottimizzazione sulle varietà disco di Poincaré e iperboloide e capire se queste due varietà sono equivalenti ed interscambiabili, non solo nella rappresentazione dello spazio iperbolico ma anche come varietà su cui effettuare ottimizzazione.\\
Gli esperimenti effettuati si suddividono in due parti che nel seguito descriveremo. Per effettuare gli esperimenti ci siamo muniti di un dataset di circa duecento istanze del problema, prendendo perciò un numero fissato, comune a tutte le istanze, di punti randomici sul disco (sampling fatto sfruttando la funzione $rand$ del file $PoincareFactory$ della libreria manopt), per ogni istanza abbiamo calcolato il punto limite tramite l'algoritmo a passo fisso con un $\alpha$ molto piccolo e come guardia di arresto abbiamo scelto la condizione che la norma euclidea del gradiente $g_k$ scendesse sotto un valore soglia (e.g. $10^{-8}$).
\section{Appendice}
\subsection{Varietà}
Sia $M$ un insieme. Una bigiezione $\phi$ di un sottoinsime $U$ di $M$ in un sottoinsieme aperto di $R^d$ è chiamata mappa d-dimensionale dell'insieme $U$. Data una carta $(U, \psi)$ e $x \in U$ gli elementi di $\psi(x) \in R^d$ sono le coordinate di $x$ nella carta $(U, \psi)$. Un $Atlas$ $(A)$ di $M$ in $R^d$ è un insieme di carte $(U_a, \psi_a)$ dell'insieme $M$ tali che:
\begin{itemize}
    \item $\bigcup_a U_a = M$
    \item Per ogni coppia $\alpha, \beta$ con $U_{\alpha} \bigcap U_{\beta} \neq \emptyset$ gli insiemi $\psi_{\alpha}(U_{\alpha} \bigcap U_{\beta})$ e $\psi_{\beta}(U_{\alpha} \bigcap U_{\beta})$ sono sottoinsiemi aperti di $R^d$ ed il cambio di coordinate $\psi_{\alpha} \circ \psi_{\beta}^{-1}:$ $R^d \to R^d$ è $smooth$.
\end{itemize}
Dato un'atlas $A$, sia $A^+$ l'insieme delle carte $(U, \psi)$ tali che $A \bigcup \{(U, \psi)\}$ è ancora un'atlas. $A^+$ è un'atlas massimale generato da $A$. Un'atlas massimale dell'insieme $M$ è chiamato struttra differenziabile su $M$.
\begin{definition}
Una varietà d-dimensionale è una coppia $(M, A^+)$, dove $M$ è un insieme e $A^+$ è un atlas massimale su $M$ in $R^d$, tale che la topologia indotta da $A^+$ è di hausdorff.
\end{definition}
Data una carta $\psi$ su $M$, l'inverso $\psi^{-1}$ è chiamata parametrizzazione locale di $M$.
\subsubsection{Vettori Tangenti}
Riprendendo il concetto di derivata direzionale possiamo generalizzarla per un funzione $f$ definita su un manifold, sustituendo $t \mapsto (x + t\eta)$ con una curva su $M$ attraverso $x \in M$ (punto di derivazione) $(\gamma (0) = x)$ questo porta ad una derivata direzionale ben definita $\frac{\partial f(\gamma (t))}{\partial t} \vert_{t=0}$. Formalmente, sia $M$ una varietà e $\gamma$ una mappa differenziabile $\gamma : R \to M | t \mapsto \gamma (t)$ detta curva in $M$, data, una funzione $f$ a valori reali definita su $M$, la funzione $f \circ \gamma : t \mapsto f(\gamma (t))$ è una funzione differenziabile da $R$ in $R$ con una ben definita deerivata.\\
Sia $x \in M$; $\gamma (0) = x$ allora $F_{x}(M)$ è l'insieme delle funzioni reali differenziabili definite in un intorno di $x$. Il mapping $\dot{\gamma} (0)$ da $F_x(M)$ in $R$ è definito come: $\dot{\gamma} (0) f = \frac{\partial f(\gamma (t))}{\partial t} \vert_{t=0}$ con $f \in F_x(M)$; é chiamato vettore tangente alla curva $\gamma$ in $t=0$.
\begin{definition}[Vettore Tangente]
Un vettore tangente $\epsilon_x$ ad una varietà $M$ in un punto $x \in M$ è una mappa da $F_x(M)$ in $R$ tale ceh esiste una curva $\gamma$ a valori in $M$ con $\gamma (0) = x$ e che soddisfa:\\
$\epsilon_xf = \dot{\gamma (0)} f = \frac{\partial f(\gamma (t))}{\partial t} \vert_{t=0} \quad \forall f \in F_x(M)$.\\
Tale curva $\gamma$ è detta realizzare il vettore tangente $\epsilon_x$
\end{definition}
\begin{definition}[Spazio Tangente]
Uno spazio tangente ad $M$ in $x$ denominato come $T_xM$ è l'insieme di tutti i vettori tangenti ad $M$ in $x$. Questo insieme ammette una struttura di spazio vettoriale come segue: date $\dot{\gamma}_1 (0)$ e $\dot{\gamma}_2 (0)$ in $T_xM$ e $a, b$ in $R$ allora $(a \dot{\gamma}_1 (0) + b \dot{\gamma}_2 (0)) f = a (\dot{\gamma}_1 (0) f) + b (\dot{\gamma}_2 (0) f)$
\end{definition}
Lo spazio tangente $T_xM$ mette a disposizione un'approssimazione locale di spazio vettoriale della varietà. Perciò tramite le Retrazioni possiamo effettuare ottimizzazione sullo spazio vettoriale $T_xM$.
\begin{definition}[Tangent Bundle]
Data una varietà $M$, sia $TM$ l'insieme di tutti i vettori tangenti ad $M$.\\
$TM = \bigcup_{x \in M} T_xM$.
\end{definition}
Sia $P:M \to N$ una mappa liscia tra due varietà $M$ ed $N$. sia $\epsilon_x$ un vettore tangente in un punto $x$ di $M$, si può far vedere che la mappa $DP(x)[\epsilon_x]$ da $F_{P(x)}(N)$ in $R$ è definita da:\\
$(DP(x)[\epsilon_x])f = \epsilon_x(f \circ P)$\\
è un vettore tangente ad $N$ in $P(x)$.\\
La mappa $DP(X) : T_xM \to T_{P(x)}N | \epsilon_x \mapsto DP(x)[\epsilon_x]$ è una mappa lineare chiamata \emph{differenziale} di P in x.
\subsubsection{Metriche distanze e e gradienti Riemanniani}
I vettori tangenti ad una varietà generalizzano la nozione di derivata direzionale. Questo può essere fatto munendo lo spazio tangente $T_xM$ con un prodotto interno $\langle \cdot, \cdot \rangle_x$ bilineare, definito positivo e simmetrico. Il prodotto interno induce una norma $\parallel \epsilon_x \parallel_x = \sqrt{\langle \epsilon_x, \epsilon_x \rangle_x}$ sun $T_xM$. Una varietà il cui spazio tangente è munito di un prodotto interno prende il nome di varietà riemanniana $(M, g)$ con $g$ metrica riemanniana.\\
La lunghezza di una curva $\gamma:[a,b] \to M$ in una varietà riemanniana $(M, g)$ è definita da: $L(\gamma) = \int_a^b \sqrt{g(\dot{\gamma (t)}, g(\dot{\gamma (t)}))}$\\
La distanza riemanniana in una varietà riemanniana connessa $(M,g)$ è $dist: MxM \to R | dist(x,y) \mapsto inf_{\gamma \in \Gamma}(L(\gamma))$, dove $\Gamma$ è l'insieme di tutte le curve in $M$ che connettono $x$ ed $y$.\\
Sia $f$ la solita funzione liscia definita su $(M,g)$, il gradiente di $f$ in $x$ è definito come l'elemento di $T_xM$ che soddisfa: $\langle f(x), \epsilon \rangle_x = Df(x)[\epsilon]$.\\
Sia $M$ una sottovarietà immersa di una varietà riemanniana $\hat{M}$ siccome ogni spazio tangente $T_xM$ può essere trattato come sottospazio di $T_x\hat{M}$ la metrica $\hat{g}$ di $\hat{M}$ induce una metrica $g$ su $M$, $g_x(\epsilon, c) = \hat{g}_x(\epsilon, c)$ con $\epsilon, c \in T_xM$, perciò $M$ è una sotto varietà riemanniana.
\subsection{Topologia}
Una topologia su un insieme $X$ è una collezione $T$ di sottoinsiemi di $X$ chiamati insiemi aperti, tali che:
\begin{itemize}
    \item $X$ e $\emptyset$ appartengono a $T$
    \item L'unione di elementi di qualsiasi sottoinsieme di $T$ è in $T$
    \item L'intersezione di elementi di qualsiasi sotto collezione di $T$ è in $T$
\end{itemize}
Uno spazio topologico è una coppia $(X,T)$ dove $X$ è un insieme e $T$ è una topologia su $X$.
\begin{definition}[Hausdorff]
Un insieme $X$ è $T_2$ o di Hausdorff se ogni coppia di punti distinti di $X$ hanno un intorno distinto.
\end{definition}
Se $X$ è di Hausdorff allora ogni sequenza di punti di $X$ converge ad al più un punto di $X$.
\subsection{Definizioni Utili}
\begin{definition}[Positività]
Sia $A \in R^{n*n}$ è definita positiva se è simmetrica e se $v'Av > 0$ se $v$ è un vettore $\in R^n - {0}$, con $v = 0$ è diretto che $v'Av = 0$.
\end{definition}
\begin{definition}[Convessità]
Una funzione $f:\Omega \to R$ con $\Omega \leq V$, $V$ (spazio vettoriale) allora è definita convessa se: $(f((\lambda)v + (1 - \lambda)w) \leq \lambda f(v) + (1 - \lambda)f(w)$, per $\lambda \in [0;1]$ e $v, w \in \Omega$.
\end{definition}
\begin{definition}[Derivate Parziali]
Data una funzione $f:V \to R$, $V$ (spazio vettoriale), allora una derivata parziale su $x \in V$ è definita come:\\
$\frac{\partial f(x)}{\partial x_i}$ = $\lim_{h \to 0} \frac{f(x + h e_i) - f(x)}{h}$; con $x_i$ componente i-esima di $x$ ed $e_i$ i-esimo vettore della base canonica.\\
Se una funzione è differenziabile in $x$ allora tutte le derivate parziali esistono in $x$.
\end{definition}
\begin{definition}[Punto di accumulazione]
Diciamo che $x_0 \in R$ è un punto di accumulazione per un insieme $E \subset R$ se comunque scelto un intorno $B(x_0, \epsilon)$, risulta che $B(x_0, \epsilon)$ contiene almeno un punto di $E$ diverso da $x_0$.
\end{definition}
\begin{definition}[Gradiente]
Consideriamo una funzione $f$ definita su un insieme aperto $A \subset R^n$ e sia $x \in A$ se esistono in $x$ le derivate parziali rispetto ad $\{x_i\}_{i=1...n}$ allora è possibile costruire un vettore che ha per componenti le derivate perziali, perciò il gradiente è definito come: $\nabla f = [\frac{\partial f(x)}{\partial x_1}, ..., \frac{\partial f(x)}{\partial x_n}]$, inoltre per il teorema della formula del gradiente sappiamo che, dato un versore $v$ esiste la derivata direzionale $f_v$ in $x$ e che $f_v(x) = \nabla f(x) \cdot v$.
\end{definition}
\subsubsection{Teorema di Taylor}
Il teorema di Taylor è un teorema che fornisce una sequenza di approssimazioni di una funzione differenziabile attorno ad un dato punto mediante i polinomi di taylor, ovvero polinomi i cui coefficienti dipendono solo dalle derivate della funzione nel punto.
Diamo un esempio pratico di approssimazione di taylor del secondo ordine di una funzione a due variabili.\\\\
$f(x_0+h, y_0+k) = f(x_0, y_0) + f_x(x_0, y_0)h + f_y(x_0, y_0)k + \frac{1}{2}[f_{xx}(x_0, y_0)h^2 + f_{xy}(x_0, y_0)hk + f_{yy}(x_0, y_0)k^2] + R(h, k)$; con $R(h, k) \in o(\parallel (h, k) \parallel^2)$.
\end{document}